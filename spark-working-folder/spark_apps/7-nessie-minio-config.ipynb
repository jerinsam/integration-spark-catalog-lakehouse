{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61aad15-e4d6-4f5a-8e08-fc1db0c578d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ccd36bb-4e21-4352-98f4-55df269b6214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparksql_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext sparksql_magic\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db51e2-6f41-45bf-9a7b-f7e44180cf76",
   "metadata": {},
   "source": [
    "#### Following code is used to initilize the Spark Session. Delta lake package is used while creating the spark session, which will help to save spark dataframe as Delta Table. Nessie is used as default catalog and Minio as Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5241b61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################### Initilize Spark Session #########################\n",
    "\n",
    "# Minio Location; Create bucket (\"nessiebucket\") in Minio \n",
    "v_minioWarehouse = \"s3://nessiebucket/\"     \n",
    "\n",
    "# Minio URI; Use IP address in the URI not the docker service \"minio\".\n",
    "    # Using docker service \"minio\" in URI like \"http://minio:9000\" will give error \"connection closed quitely\".\n",
    "    # Adding Minio S3 endpoint in Nessie requires IP in the URI not docker service \"minio\".\n",
    "    # If AWS SDK error occurs stating \"connection closed quitely\" then it means Minio connection is not successful.\n",
    "# Use docker inspect <Minio-Container_id> to get the IP \n",
    "v_minioStorageUri = \"http://172.19.0.2:9000\"\n",
    "\n",
    "# Nessie Server URI\n",
    "v_nessieCatalogUri = \"http://nessie:19120/api/v1\"  \n",
    "\n",
    "\n",
    "# Spark Configuration\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('nessie-app')\n",
    "        .setMaster(\"spark://spark-master:7077\")\n",
    "        # Add Jars - Iceberg, AWS bundle for Minio and Nessie\n",
    "        .set('spark.jars.packages', 'org.postgresql:postgresql:42.7.3,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,software.amazon.awssdk:bundle:2.20.147,software.amazon.awssdk:url-connection-client:2.20.147')\n",
    "        # Add Iceberg and Nessie extensions\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configure Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', v_nessieCatalogUri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Configure Minio storage; Credetials and Region is required to connect to Minio will be stored in Environment Variable \n",
    "            # AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', v_minioStorageUri)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', v_minioWarehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d50f5cef-634f-447b-840c-3d24785da0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfa4f79-76b1-49c3-95a7-89ed84c111e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get spark session config details\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf7dd13b-e3cc-4962-8f20-9e2e552b04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame creation\n",
    "data = [(\"Jerin\", 29), (\"Aayush\", 35), (\"Neeraj\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a538f060-489d-4f68-87ed-22b7e29ab06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Jerin| 29|\n",
      "|Aayush| 35|\n",
      "|Neeraj| 28|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6cb16bb-c0aa-426c-b28d-c85db91965ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the \"testingnessie\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE nessie.nessieNamespace;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642a806a-cf1a-4a2d-81b4-34764b6fe118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"nessie.nessieNamespace.test_iceberg_table1\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a207eba-a61f-4548-a424-f0e65cb9613d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
