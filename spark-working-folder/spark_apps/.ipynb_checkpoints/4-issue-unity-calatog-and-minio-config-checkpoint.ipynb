{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61aad15-e4d6-4f5a-8e08-fc1db0c578d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccd36bb-4e21-4352-98f4-55df269b6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db51e2-6f41-45bf-9a7b-f7e44180cf76",
   "metadata": {},
   "source": [
    "#### Following code is used to initilize the Spark Session. Delta lake package is used while creating the spark session, which will help to save spark dataframe as Delta Table. Unity Catalog is used as default catalog instead of Delta Catalog and Minio as Object Storage\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5241b61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: unrecognized option: p\n",
      "BusyBox v1.37.0 (2024-12-13 21:18:49 UTC) multi-call binary.\n",
      "\n",
      "Usage: ps [-o COL1,COL2=HEADER] [-T]\n",
      "\n",
      "Show list of processes\n",
      "\n",
      "\t-o COL1,COL2=HEADER\tSelect columns for display\n",
      "\t-T\t\t\tShow threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "io.unitycatalog#unitycatalog-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5aab6557-6514-405d-9b39-6e144b4a959a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound io.unitycatalog#unitycatalog-spark_2.12;0.2.0 in central\n",
      "\tfound io.unitycatalog#unitycatalog-client;0.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.13 in central\n",
      "\tfound org.apache.logging.log4j#log4j-slf4j2-impl;2.23.1 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.23.1 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.23.1 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 in central\n",
      "\tfound org.openapitools#jackson-databind-nullable;0.2.6 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.0 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.0 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;6.5.1 in central\n",
      "\tfound org.antlr#antlr4;4.9.3 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.3.1 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;69.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.4 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.147 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.147 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.147 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.147 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.147 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.147 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.0/delta-spark_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.3.0!delta-spark_2.12.jar (13352ms)\n",
      "downloading https://repo1.maven.org/maven2/io/unitycatalog/unitycatalog-spark_2.12/0.2.0/unitycatalog-spark_2.12-0.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.unitycatalog#unitycatalog-spark_2.12;0.2.0!unitycatalog-spark_2.12.jar (637ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (2118ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.4/hadoop-common-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;3.3.4!hadoop-common.jar (10153ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.20.147/bundle-2.20.147.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#bundle;2.20.147!bundle.jar (491005ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.20.147/url-connection-client-2.20.147.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#url-connection-client;2.20.147!url-connection-client.jar (647ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.0/delta-storage-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.3.0!delta-storage.jar (699ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (1732ms)\n",
      "downloading https://repo1.maven.org/maven2/io/unitycatalog/unitycatalog-client/0.2.0/unitycatalog-client-0.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.unitycatalog#unitycatalog-client;0.2.0!unitycatalog-client.jar (1475ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.13/slf4j-api-2.0.13.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.13!slf4j-api.jar (703ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-slf4j2-impl/2.23.1/log4j-slf4j2-impl-2.23.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.logging.log4j#log4j-slf4j2-impl;2.23.1!log4j-slf4j2-impl.jar (617ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.23.1/log4j-api-2.23.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.logging.log4j#log4j-api;2.23.1!log4j-api.jar (1507ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.15.0/jackson-databind-2.15.0.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.15.0!jackson-databind.jar (6296ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.15.0/jackson-module-scala_2.12-2.15.0.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.0!jackson-module-scala_2.12.jar(bundle) (1933ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.15.0/jackson-annotations-2.15.0.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.15.0!jackson-annotations.jar (711ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.15.0/jackson-core-2.15.0.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.15.0!jackson-core.jar (2103ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.15.0/jackson-dataformat-xml-2.15.0.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0!jackson-dataformat-xml.jar (895ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.9.3/antlr4-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.9.3!antlr4.jar (4991ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.0/hadoop-client-runtime-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.0!hadoop-client-runtime.jar (51573ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.17.0/jackson-datatype-jsr310-2.17.0.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0!jackson-datatype-jsr310.jar(bundle) (960ms)\n",
      "downloading https://repo1.maven.org/maven2/org/openapitools/jackson-databind-nullable/0.2.6/jackson-databind-nullable-0.2.6.jar ...\n",
      "\t[SUCCESSFUL ] org.openapitools#jackson-databind-nullable;0.2.6!jackson-databind-nullable.jar (599ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (615ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.23.1/log4j-core-2.23.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.logging.log4j#log4j-core;2.23.1!log4j-core.jar (5756ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (617ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.woodstox#stax2-api;4.2.1!stax2-api.jar(bundle) (1014ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/6.5.1/woodstox-core-6.5.1.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.woodstox#woodstox-core;6.5.1!woodstox-core.jar(bundle) (3330ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (730ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.3.1/ST4-4.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.3.1!ST4.jar (849ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (617ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (641ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/69.1/icu4j-69.1.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;69.1!icu4j.jar (24078ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.0/hadoop-client-api-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.0!hadoop-client-api.jar (38223ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.4/snappy-java-1.1.10.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.4!snappy-java.jar(bundle) (7211ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.2!commons-logging.jar (699ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (656673ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (1322ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1!hadoop-shaded-protobuf_3_7.jar (3032ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.3.4/hadoop-annotations-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.3.4!hadoop-annotations.jar (672ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1!hadoop-shaded-guava.jar (10235ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;27.0-jre!guava.jar(bundle) (6074ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (609ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (3381ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.13!httpclient.jar (1937ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.15!commons-codec.jar (1082ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.8.0/commons-io-2.8.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.8.0!commons-io.jar (997ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.6/commons-net-3.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-net#commons-net;3.6!commons-net.jar (1884ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-collections#commons-collections;3.2.2!commons-collections.jar (1578ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet#javax.servlet-api;3.1.0!javax.servlet-api.jar (1045ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.43.v20210629/jetty-server-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-server;9.4.43.v20210629!jetty-server.jar (1921ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.43.v20210629/jetty-util-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.4.43.v20210629!jetty-util.jar (1642ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-servlet/9.4.43.v20210629/jetty-servlet-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-servlet;9.4.43.v20210629!jetty-servlet.jar (763ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-webapp/9.4.43.v20210629/jetty-webapp-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-webapp;9.4.43.v20210629!jetty-webapp.jar (1161ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.19!jersey-core.jar (1590ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-servlet;1.19!jersey-servlet.jar (795ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-json;1.19!jersey-json.jar (918ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-server;1.19!jersey-server.jar (2078ms)\n",
      "downloading https://repo1.maven.org/maven2/ch/qos/reload4j/reload4j/1.2.22/reload4j-1.2.22.jar ...\n",
      "\t[SUCCESSFUL ] ch.qos.reload4j#reload4j;1.2.22!reload4j.jar (1393ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar ...\n",
      "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.4!commons-beanutils.jar (1015ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-configuration2;2.1.1!commons-configuration2.jar (2004ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.12.0!commons-lang3.jar (2212ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-text/1.4/commons-text-1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-text;1.4!commons-text.jar (1063ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-reload4j;1.7.36!slf4j-reload4j.jar (564ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (1742ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/re2j/re2j/1.1/re2j-1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.re2j#re2j;1.1!re2j.jar (1162ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (2272ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.8.9/gson-2.8.9.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.8.9!gson.jar (1268ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/3.3.4/hadoop-auth-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;3.3.4!hadoop-auth.jar (971ms)\n",
      "downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar ...\n",
      "\t[SUCCESSFUL ] com.jcraft#jsch;0.1.55!jsch.jar (2343ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-client;4.2.0!curator-client.jar(bundle) (13159ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle) (1166ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar (2304ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.21!commons-compress.jar (2247ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-core;1.0.1!kerb-core.jar (1117ms)\n",
      "downloading https://repo1.maven.org/maven2/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar ...\n",
      "\t[SUCCESSFUL ] dnsjava#dnsjava;2.1.7!dnsjava.jar (1223ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#failureaccess;1.0!failureaccess.jar (569ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava!listenablefuture.jar (572ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;2.5.2!checker-qual.jar (1149ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.1!j2objc-annotations.jar (566ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.17!animal-sniffer-annotations.jar (549ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.13!httpcore.jar (1050ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.43.v20210629/jetty-http-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-http;9.4.43.v20210629!jetty-http.jar (1183ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.43.v20210629/jetty-io-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-io;9.4.43.v20210629!jetty-io.jar (941ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-security/9.4.43.v20210629/jetty-security-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-security;9.4.43.v20210629!jetty-security.jar (762ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.43.v20210629/jetty-util-ajax-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629!jetty-util-ajax.jar (678ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-xml/9.4.43.v20210629/jetty-xml-9.4.43.v20210629.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-xml;9.4.43.v20210629!jetty-xml.jar (1712ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.ws.rs#jsr311-api;1.1.1!jsr311-api.jar (685ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (728ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (3238ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (1629ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (3290ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.9.13!jackson-jaxrs.jar (619ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.9.13!jackson-xc.jar (704ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...\n",
      "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (861ms)\n",
      "downloading https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/9.8.1/nimbus-jose-jwt-9.8.1.jar ...\n",
      "\t[SUCCESSFUL ] com.nimbusds#nimbus-jose-jwt;9.8.1!nimbus-jose-jwt.jar (2081ms)\n",
      "downloading https://repo1.maven.org/maven2/net/minidev/json-smart/2.4.7/json-smart-2.4.7.jar ...\n",
      "\t[SUCCESSFUL ] net.minidev#json-smart;2.4.7!json-smart.jar(bundle) (873ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle) (1406ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-simplekdc;1.0.1!kerb-simplekdc.jar (635ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (578ms)\n",
      "downloading https://repo1.maven.org/maven2/net/minidev/accessors-smart/2.4.7/accessors-smart-2.4.7.jar ...\n",
      "\t[SUCCESSFUL ] net.minidev#accessors-smart;2.4.7!accessors-smart.jar(bundle) (609ms)\n",
      "downloading https://repo1.maven.org/maven2/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.ow2.asm#asm;5.0.4!asm.jar (656ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar (1049ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.yetus#audience-annotations;0.5.0!audience-annotations.jar (575ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-client;1.0.1!kerb-client.jar (723ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-admin;1.0.1!kerb-admin.jar (694ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-config;1.0.1!kerby-config.jar (595ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-common;1.0.1!kerb-common.jar (643ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-util;1.0.1!kerb-util.jar (601ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#token-provider;1.0.1!token-provider.jar (577ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-pkix;1.0.1!kerby-pkix.jar (955ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-asn1;1.0.1!kerby-asn1.jar (688ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-util;1.0.1!kerby-util.jar (687ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-crypto;1.0.1!kerb-crypto.jar (689ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-server;1.0.1!kerb-server.jar (648ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-xdr;1.0.1!kerby-xdr.jar (583ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-identity;1.0.1!kerb-identity.jar (567ms)\n",
      "downloading https://repo1.maven.org/maven2/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar ...\n",
      "\t[SUCCESSFUL ] jakarta.activation#jakarta.activation-api;1.2.1!jakarta.activation-api.jar (597ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (674ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/eventstream/eventstream/1.0.1/eventstream-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.eventstream#eventstream;1.0.1!eventstream.jar (599ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/utils/2.20.147/utils-2.20.147.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#utils;2.20.147!utils.jar (926ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/annotations/2.20.147/annotations-2.20.147.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#annotations;2.20.147!annotations.jar (561ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/http-client-spi/2.20.147/http-client-spi-2.20.147.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#http-client-spi;2.20.147!http-client-spi.jar (666ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.4!reactive-streams.jar (586ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/metrics-spi/2.20.147/metrics-spi-2.20.147.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#metrics-spi;2.20.147!metrics-spi.jar (603ms)\n",
      ":: resolution report :: resolve 244535ms :: artifacts dl 1463710ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-scala_2.12;2.15.0 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;6.5.1 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;69.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\tio.unitycatalog#unitycatalog-client;0.2.0 from central in [default]\n",
      "\tio.unitycatalog#unitycatalog-spark_2.12;0.2.0 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.3.1 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.9.3 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.23.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.23.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-slf4j2-impl;2.23.1 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.openapitools#jackson-databind-nullable;0.2.6 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.13 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.4 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.147 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.147 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.147 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.147 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.147 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.147 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;2.0.9 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.17.0 by [com.fasterxml.jackson.core#jackson-annotations;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.17.0 by [com.fasterxml.jackson.core#jackson-core;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.17.0 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.0-rc2 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 by [com.thoughtworks.paranamer#paranamer;2.8] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.10.4] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 by [com.fasterxml.woodstox#woodstox-core;6.5.1] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.13] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  135  |  123  |  123  |   12  ||  123  |  123  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5aab6557-6514-405d-9b39-6e144b4a959a\n",
      "\tconfs: [default]\n",
      "\t123 artifacts copied, 0 already retrieved (879053kB/14178ms)\n",
      "25/01/21 09:16:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "# .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.access.key\", \"root\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.secret.key\", \"jerinminioserver\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "# ,software.amazon.awssdk:bundle:2.20.147,software.amazon.awssdk:url-connection-client:2.20.147\n",
    "\n",
    "# .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.access.key\", \"root\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.secret.key\", \"jerinminioserver\") \\\n",
    "# .config('spark.sql.catalog.unity.io-impl', 'software.amazon.awssdk.services.s3.S3Client') \\\n",
    "\n",
    "# Minio Location; Create bucket (\"ucbucket\") in Minio \n",
    "v_minioWarehouse = \"s3://ucbucket/\"     \n",
    "\n",
    "# Minio URI; Use IP address in the URI not the docker service \"minio\".\n",
    "v_minioStorageUri = \"http://minio:9000\"\n",
    "\n",
    "# Nessie Server URI\n",
    "v_ucCatalogUri = \"http://unity-catalog:8080\"  \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"jupyter-unity-catalog-minio-session\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"io.delta:delta-spark_2.12:3.3.0,\"\n",
    "            \"io.unitycatalog:unitycatalog-spark_2.12:0.2.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"org.apache.hadoop:hadoop-common:3.3.4,\"\n",
    "            \"software.amazon.awssdk:bundle:2.20.147,\"\n",
    "            \"software.amazon.awssdk:url-connection-client:2.20.147\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"io.unitycatalog.spark.UCSingleCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.unity\", \"io.unitycatalog.spark.UCSingleCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.unity.uri\", \"http://unity-catalog:8080\") \\\n",
    "    .config(\"spark.sql.catalog.unity.token\", \"\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"unity\") \\\n",
    "    .config('spark.sql.catalog.unity.s3.endpoint', v_minioStorageUri) \\\n",
    "    .config('spark.sql.catalog.unity.warehouse', v_minioWarehouse) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50f5cef-634f-447b-840c-3d24785da0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5dfa4f79-76b1-49c3-95a7-89ed84c111e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.jars',\n",
       "  'file:///root/.ivy2/jars/io.delta_delta-spark_2.12-3.2.0.jar,file:///root/.ivy2/jars/io.unitycatalog_unitycatalog-spark_2.12-0.2.0.jar,file:///root/.ivy2/jars/io.delta_delta-storage-3.2.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///root/.ivy2/jars/io.unitycatalog_unitycatalog-client-0.2.0.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.13.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.23.1.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.23.1.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.12-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.9.3.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar,file:///root/.ivy2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-core-2.23.1.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///root/.ivy2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar,file:///root/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.3.1.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-69.1.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.2.jar'),\n",
       " ('spark.app.id', 'app-20250103065338-0003'),\n",
       " ('spark.app.startTime', '1735887217085'),\n",
       " ('spark.app.submitTime', '1735887216550'),\n",
       " ('spark.files',\n",
       "  'file:///root/.ivy2/jars/io.delta_delta-spark_2.12-3.2.0.jar,file:///root/.ivy2/jars/io.unitycatalog_unitycatalog-spark_2.12-0.2.0.jar,file:///root/.ivy2/jars/io.delta_delta-storage-3.2.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///root/.ivy2/jars/io.unitycatalog_unitycatalog-client-0.2.0.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.13.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.23.1.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.23.1.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.12-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.9.3.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar,file:///root/.ivy2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-core-2.23.1.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///root/.ivy2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar,file:///root/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.3.1.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-69.1.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.2.jar'),\n",
       " ('spark.master', 'spark://spark-master:7077'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.catalog.unity.token', ''),\n",
       " ('spark.driver.port', '41263'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/root/.ivy2/jars/io.delta_delta-spark_2.12-3.2.0.jar,/root/.ivy2/jars/io.unitycatalog_unitycatalog-spark_2.12-0.2.0.jar,/root/.ivy2/jars/io.delta_delta-storage-3.2.0.jar,/root/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,/root/.ivy2/jars/io.unitycatalog_unitycatalog-client-0.2.0.jar,/root/.ivy2/jars/org.slf4j_slf4j-api-2.0.13.jar,/root/.ivy2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.23.1.jar,/root/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.23.1.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar,/root/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.12-2.15.0.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar,/root/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar,/root/.ivy2/jars/org.antlr_antlr4-4.9.3.jar,/root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar,/root/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar,/root/.ivy2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar,/root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,/root/.ivy2/jars/org.apache.logging.log4j_log4j-core-2.23.1.jar,/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,/root/.ivy2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar,/root/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar,/root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,/root/.ivy2/jars/org.antlr_ST4-4.3.1.jar,/root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,/root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,/root/.ivy2/jars/com.ibm.icu_icu4j-69.1.jar,/root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar,/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar,/root/.ivy2/jars/commons-logging_commons-logging-1.2.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', 'file:/opt/spark/apps/spark-warehouse'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'spark://2c03a8ef8999:41263/files/org.slf4j_slf4j-api-2.0.13.jar,spark://2c03a8ef8999:41263/files/org.apache.hadoop_hadoop-client-api-3.4.0.jar,spark://2c03a8ef8999:41263/files/io.delta_delta-spark_2.12-3.2.0.jar,spark://2c03a8ef8999:41263/files/io.unitycatalog_unitycatalog-spark_2.12-0.2.0.jar,spark://2c03a8ef8999:41263/files/io.unitycatalog_unitycatalog-client-0.2.0.jar,spark://2c03a8ef8999:41263/files/commons-logging_commons-logging-1.2.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.jackson.core_jackson-core-2.15.0.jar,spark://2c03a8ef8999:41263/files/com.google.code.findbugs_jsr305-3.0.2.jar,spark://2c03a8ef8999:41263/files/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar,spark://2c03a8ef8999:41263/files/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,spark://2c03a8ef8999:41263/files/org.apache.logging.log4j_log4j-api-2.23.1.jar,spark://2c03a8ef8999:41263/files/org.antlr_antlr-runtime-3.5.2.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar,spark://2c03a8ef8999:41263/files/org.antlr_antlr4-4.9.3.jar,spark://2c03a8ef8999:41263/files/org.glassfish_javax.json-1.0.4.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.jackson.module_jackson-module-scala_2.12-2.15.0.jar,spark://2c03a8ef8999:41263/files/org.apache.logging.log4j_log4j-core-2.23.1.jar,spark://2c03a8ef8999:41263/files/org.xerial.snappy_snappy-java-1.1.10.4.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.woodstox_woodstox-core-6.5.1.jar,spark://2c03a8ef8999:41263/files/org.antlr_antlr4-runtime-4.9.3.jar,spark://2c03a8ef8999:41263/files/com.ibm.icu_icu4j-69.1.jar,spark://2c03a8ef8999:41263/files/org.apache.logging.log4j_log4j-slf4j2-impl-2.23.1.jar,spark://2c03a8ef8999:41263/files/io.delta_delta-storage-3.2.0.jar,spark://2c03a8ef8999:41263/files/org.antlr_ST4-4.3.1.jar,spark://2c03a8ef8999:41263/files/com.thoughtworks.paranamer_paranamer-2.8.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar,spark://2c03a8ef8999:41263/files/org.openapitools_jackson-databind-nullable-0.2.6.jar,spark://2c03a8ef8999:41263/files/org.codehaus.woodstox_stax2-api-4.2.1.jar,spark://2c03a8ef8999:41263/files/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar'),\n",
       " ('spark.sql.catalog.unity', 'io.unitycatalog.spark.UCSingleCatalog'),\n",
       " ('spark.app.name', 'unity-catalog-pyspark-session'),\n",
       " ('spark.history.fs.logDirectory', '/opt/spark/spark-events'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.sql.catalog.spark_catalog', 'io.unitycatalog.spark.UCSingleCatalog'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.host', '2c03a8ef8999'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://2c03a8ef8999:41263/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.jackson.module_jackson-module-scala_2.12-2.15.0.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar,spark://2c03a8ef8999:41263/jars/org.antlr_antlr-runtime-3.5.2.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar,spark://2c03a8ef8999:41263/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,spark://2c03a8ef8999:41263/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar,spark://2c03a8ef8999:41263/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.23.1.jar,spark://2c03a8ef8999:41263/jars/org.apache.logging.log4j_log4j-core-2.23.1.jar,spark://2c03a8ef8999:41263/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar,spark://2c03a8ef8999:41263/jars/io.unitycatalog_unitycatalog-client-0.2.0.jar,spark://2c03a8ef8999:41263/jars/org.antlr_antlr4-4.9.3.jar,spark://2c03a8ef8999:41263/jars/com.google.code.findbugs_jsr305-3.0.2.jar,spark://2c03a8ef8999:41263/jars/com.ibm.icu_icu4j-69.1.jar,spark://2c03a8ef8999:41263/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar,spark://2c03a8ef8999:41263/jars/io.delta_delta-storage-3.2.0.jar,spark://2c03a8ef8999:41263/jars/io.unitycatalog_unitycatalog-spark_2.12-0.2.0.jar,spark://2c03a8ef8999:41263/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar,spark://2c03a8ef8999:41263/jars/org.antlr_ST4-4.3.1.jar,spark://2c03a8ef8999:41263/jars/org.apache.logging.log4j_log4j-api-2.23.1.jar,spark://2c03a8ef8999:41263/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar,spark://2c03a8ef8999:41263/jars/commons-logging_commons-logging-1.2.jar,spark://2c03a8ef8999:41263/jars/org.antlr_antlr4-runtime-4.9.3.jar,spark://2c03a8ef8999:41263/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,spark://2c03a8ef8999:41263/jars/io.delta_delta-spark_2.12-3.2.0.jar,spark://2c03a8ef8999:41263/jars/org.slf4j_slf4j-api-2.0.13.jar,spark://2c03a8ef8999:41263/jars/org.glassfish_javax.json-1.0.4.jar'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///root/.ivy2/jars/io.delta_delta-spark_2.12-3.2.0.jar,file:///root/.ivy2/jars/io.unitycatalog_unitycatalog-spark_2.12-0.2.0.jar,file:///root/.ivy2/jars/io.delta_delta-storage-3.2.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///root/.ivy2/jars/io.unitycatalog_unitycatalog-client-0.2.0.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.13.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.23.1.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.23.1.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.12-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar,file:///root/.ivy2/jars/org.antlr_antlr4-4.9.3.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar,file:///root/.ivy2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-core-2.23.1.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///root/.ivy2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar,file:///root/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar,file:///root/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar,file:///root/.ivy2/jars/org.antlr_ST4-4.3.1.jar,file:///root/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar,file:///root/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar,file:///root/.ivy2/jars/com.ibm.icu_icu4j-69.1.jar,file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar,file:///root/.ivy2/jars/commons-logging_commons-logging-1.2.jar'),\n",
       " ('spark.sql.catalog.unity.uri', 'http://unity-catalog:8080'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.eventLog.dir', '/opt/spark/spark-events'),\n",
       " ('spark.sql.defaultCatalog', 'unity')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get spark session config details\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7dd13b-e3cc-4962-8f20-9e2e552b04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame creation\n",
    "data = [(\"Jerin\", 29), (\"Aayush\", 35), (\"Neeraj\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a538f060-489d-4f68-87ed-22b7e29ab06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Jerin| 29|\n",
      "|Aayush| 35|\n",
      "|Neeraj| 28|\n",
      "+------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a622f29-da1b-4954-9311-b424c54b9e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE SCHEMA demotest;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b3e7b2-db35-4639-88fb-d7050b7d82d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# unmanaged delta table - Save to minio\n",
    "df.write.format(\"delta\").save(\"s3a://ucbucket/external_tables/testUnmanagedTableOnMinionewTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d3f7ccb-3b6d-4781-9112-e12ba3612a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Aayush| 35|\n",
      "|Neeraj| 28|\n",
      "| Jerin| 29|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read delta table from minio\n",
    "spark.sql(\"\"\"select * from delta.`s3a://ucbucket/external_tables/testUnmanagedTableOnMinionewTest`;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b989920b-fb34-4425-9ede-99cca474c2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">catalog</td></tr><tr><td>spark_catalog</td></tr><tr><td>unity</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba2cae38-caea-4270-ac73-c2e21f8a529d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">namespace</td></tr><tr><td>default</td></tr><tr><td>demo</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SHOW SCHEMAS IN unity;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5ebbd4-8025-439d-86f5-7886c7819dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">namespace</td><td style=\"font-weight: bold\">tableName</td><td style=\"font-weight: bold\">isTemporary</td></tr><tr><td>default</td><td>marksheet</td><td>False</td></tr><tr><td>default</td><td>marksheet_uniform</td><td>False</td></tr><tr><td>default</td><td>numbers</td><td>False</td></tr><tr><td>default</td><td>testUnityCatalogOnMinio</td><td>False</td></tr><tr><td>default</td><td>testUnityCatalogOnMinio123456</td><td>False</td></tr><tr><td>default</td><td>user_countries</td><td>False</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SHOW TABLES IN unity.default;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1bec4e-873c-4cba-befb-0a0a189724f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "USE unity.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e92d416-a4c1-462b-ba0f-7c7675196769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">current_database()</td></tr><tr><td>default</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT current_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a601f667-63bf-49e2-90e8-3f0e4b4a9932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 07:57:10 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bucket (\"ucbucket\") in Minio, if not exists.\n",
    "# The following code will cerate the folders in Minio and will also register the Table Name in Unity Catalog. \n",
    "# However the storage location (i.e. Minio Location) in property section of registered table in Unity Catalog is incorrect as local file system directory name is added as prefix in minio location .\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE unity.demotest.testUnityCatalogOnMinionewing (\n",
    "    id INT, \n",
    "    desc STRING\n",
    ") \n",
    "USING delta \n",
    "LOCATION 's3a://ucbucket/minio_data/tables/testUnityCatalogOnMinionewing';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8edb5b5e-167c-4327-94de-69d4f1e959a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o60.sql.\n: java.lang.AssertionError: assertion failed\n\tat scala.Predef$.assert(Predef.scala:208)\n\tat io.unitycatalog.spark.UCProxy.createTable(UCSingleCatalog.scala:299)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createTable(DelegatingCatalogExtension.java:102)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createCatalogTable(DeltaCatalog.scala:328)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:373)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:350)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:341)\n\tat io.unitycatalog.spark.UCSingleCatalog.createTable(UCSingleCatalog.scala:101)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparksql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mCREATE TABLE unity.default.testUnityCatalogOnMinionewingnew (\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    id INT, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    desc STRING);\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sparksql_magic/sparksql.py:40\u001b[0m, in \u001b[0;36mSparkSql.sparksql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive spark session is not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache dataframe with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m load\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlazy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.sql.\n: java.lang.AssertionError: assertion failed\n\tat scala.Predef$.assert(Predef.scala:208)\n\tat io.unitycatalog.spark.UCProxy.createTable(UCSingleCatalog.scala:299)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createTable(DelegatingCatalogExtension.java:102)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createCatalogTable(DeltaCatalog.scala:328)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:373)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:350)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:341)\n\tat io.unitycatalog.spark.UCSingleCatalog.createTable(UCSingleCatalog.scala:101)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "CREATE TABLE unity.demotest.testUnityCatalogOnMinionewing123 (\n",
    "    id INT, \n",
    "    desc STRING);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86939784-4271-45ac-8b8b-96c4e7040b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_TABLE_NOT_FOUND] Delta table `demotest`.`testUnityCatalogOnMinionewing` doesn't exist.;\nAppendData RelationV2[] unity.demotest.testUnityCatalogOnMinionewing unity.demotest.testUnityCatalogOnMinionewing, false\n+- LocalRelation [col1#1980, col2#1981]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mInsert into unity.demotest.testUnityCatalogOnMinionewing Values (1,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJerin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),(2,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNeeraj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m), (3,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAtul\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),(4,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPriti\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),(5,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRahul\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),(6,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChinmay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),(7,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAshu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m);\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DELTA_TABLE_NOT_FOUND] Delta table `demotest`.`testUnityCatalogOnMinionewing` doesn't exist.;\nAppendData RelationV2[] unity.demotest.testUnityCatalogOnMinionewing unity.demotest.testUnityCatalogOnMinionewing, false\n+- LocalRelation [col1#1980, col2#1981]\n"
     ]
    }
   ],
   "source": [
    "# Since the location is incorrect in Unity Catalog, therefore the below code will throw error.\n",
    "spark.sql(\"\"\"\n",
    "Insert into unity.demotest.testUnityCatalogOnMinionewing Values (1,'Jerin'),(2,'Neeraj'), (3,'Atul'),(4,'Priti'),(5,'Rahul'),(6,'Chinmay'),(7,'Ashu');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa042cd4-720c-4713-9b14-c5bf9a855261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_TABLE_NOT_FOUND] Delta table `default`.`testUnityCatalogOnMinio123456` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparksql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSelect * from unity.default.testUnityCatalogOnMinio123456;\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sparksql_magic/sparksql.py:40\u001b[0m, in \u001b[0;36mSparkSql.sparksql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive spark session is not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache dataframe with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m load\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlazy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DELTA_TABLE_NOT_FOUND] Delta table `default`.`testUnityCatalogOnMinio123456` doesn't exist."
     ]
    }
   ],
   "source": [
    "# Since the location is incorrect in Unity Catalog, therefore the below code will throw error.\n",
    "%%sparksql\n",
    "\n",
    "Select * from unity.default.testUnityCatalogOnMinio123456;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05f7951-27d1-411f-b7dd-babcbb605119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.sql.\n: io.unitycatalog.client.ApiException: generateTemporaryTableCredentials call failed with: 400 - {\"error_code\":\"FAILED_PRECONDITION\",\"details\":[{\"reason\":\"FAILED_PRECONDITION\",\"metadata\":{},\"@type\":\"google.rpc.ErrorInfo\"}],\"stack_trace\":null,\"message\":\"S3 bucket configuration not found.\"}\n\tat io.unitycatalog.client.api.TemporaryCredentialsApi.getApiException(TemporaryCredentialsApi.java:78)\n\tat io.unitycatalog.client.api.TemporaryCredentialsApi.generateTemporaryTableCredentialsWithHttpInfo(TemporaryCredentialsApi.java:266)\n\tat io.unitycatalog.client.api.TemporaryCredentialsApi.generateTemporaryTableCredentials(TemporaryCredentialsApi.java:244)\n\tat io.unitycatalog.spark.UCProxy.loadTable(UCSingleCatalog.scala:266)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:73)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.super$loadTable(DeltaCatalog.scala:229)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$loadTable$1(DeltaCatalog.scala:229)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:228)\n\tat io.unitycatalog.spark.UCSingleCatalog.loadTable(UCSingleCatalog.scala:73)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparksql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSelect * from unity.default.testTableFromBash\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sparksql_magic/sparksql.py:40\u001b[0m, in \u001b[0;36mSparkSql.sparksql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive spark session is not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache dataframe with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m load\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlazy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.sql.\n: io.unitycatalog.client.ApiException: generateTemporaryTableCredentials call failed with: 400 - {\"error_code\":\"FAILED_PRECONDITION\",\"details\":[{\"reason\":\"FAILED_PRECONDITION\",\"metadata\":{},\"@type\":\"google.rpc.ErrorInfo\"}],\"stack_trace\":null,\"message\":\"S3 bucket configuration not found.\"}\n\tat io.unitycatalog.client.api.TemporaryCredentialsApi.getApiException(TemporaryCredentialsApi.java:78)\n\tat io.unitycatalog.client.api.TemporaryCredentialsApi.generateTemporaryTableCredentialsWithHttpInfo(TemporaryCredentialsApi.java:266)\n\tat io.unitycatalog.client.api.TemporaryCredentialsApi.generateTemporaryTableCredentials(TemporaryCredentialsApi.java:244)\n\tat io.unitycatalog.spark.UCProxy.loadTable(UCSingleCatalog.scala:266)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.loadTable(DelegatingCatalogExtension.java:73)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.super$loadTable(DeltaCatalog.scala:229)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$loadTable$1(DeltaCatalog.scala:229)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:228)\n\tat io.unitycatalog.spark.UCSingleCatalog.loadTable(UCSingleCatalog.scala:73)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Manually created a table using below Unity Catalog Command.This is done to check if the storage location is correctly tagged in the unity catalog table then \"will Spark be able to read the table properly?\"\n",
    "# bin/uc table create --full_name unity.default.testTableFromBash --columns \"id INT, name STRING, some_times TIMESTAMP\" --storage_location s3://ucbucket/external_tables/testUnmanagedTableOnMinio\n",
    "%%sparksql\n",
    "\n",
    "Select * from unity.default.testTableFromBash\n",
    "\n",
    "# The above code also thrown error, It seems, Unity Catalog's server.properties file should have S3 endpoint i.e. minio endpoint but as of now S3 endpoint is not supported by Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef30f53-872b-4925-bcd1-e8feead50b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
