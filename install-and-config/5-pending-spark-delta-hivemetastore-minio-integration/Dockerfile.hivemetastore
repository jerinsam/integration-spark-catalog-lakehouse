FROM python:3.11.11-bullseye AS hive-base

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      sudo \
      curl \
      vim \
      unzip \
      rsync \
      openjdk-11-jdk \
      build-essential \
      software-properties-common \
      ssh && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*



## Download spark and hadoop dependencies and install

# Optional env variables
ENV HIVE_HOME=${HIVE_HOME:-"/opt/hive"} 
ENV HADOOP_HOME=${HADOOP_HOME:-"/opt/hadoop"}
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
# Java is already installed from above step and added in PATH. JAVA_HOME is required for hadoop 
ENV JAVA_HOME=${JAVA_HOME:-"/usr/lib/jvm/java-11-openjdk-amd64"}

RUN mkdir -p ${HADOOP_HOME} && mkdir -p ${HIVE_HOME}


# Install Hive
WORKDIR ${HIVE_HOME}

RUN curl https://dlcdn.apache.org/hive/hive-4.0.1/apache-hive-4.0.1-bin.tar.gz -o apache-hive-4.0.1-bin.tar.gz \
 && tar xvzf apache-hive-4.0.1-bin.tar.gz --directory /opt/hive --strip-components 1 \
 && rm -rf apache-hive-4.0.1-bin.tar.gz

# Add Postgres DB as external metastore in Hive
COPY ./hive-config/hive-default.xml "$HIVE_HOME/conf"


# Install Hadoop
WORKDIR ${HADOOP_HOME}

RUN curl https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz -o hadoop-3.4.1.tar.gz \
 && tar xvzf hadoop-3.4.1.tar.gz --directory /opt/hadoop --strip-components 1 \
 && rm -rf hadoop-3.4.1.tar.gz


# Add Hadoop and Hive bin in Path
ENV PATH="/opt/hadoop/bin:/opt/hadoop/sbin:/opt/hive/bin:/opt/hive/sbin:${PATH}"

RUN chmod u+x /opt/hive/bin/* && \
chmod u+x /opt/hadoop/bin/* 

# Install python deps
COPY ./requirements.txt .
RUN pip3 install -r requirements.txt
    
# Define a volume
VOLUME ["/opt/hive/"]

# Create work directory 
RUN mkdir /usr/hive-metastore
RUN mkdir /usr/hive-metastore/spark-apps
RUN mkdir /usr/hive-metastore/logs

RUN chmod -R 777 /usr/hive-metastore

WORKDIR /usr/hive-metastore

# Entrypoint script - It should be declared after the WORKDIR instruction, while creating the container, entrypoint script will be executed from the Working Directory.
COPY entrypoint-hivemetastore.sh .

ENTRYPOINT ["./entrypoint-hivemetastore.sh"]


##****************** Very Important to Connect to Minio using AWS S3 Bundle Jars *************** ##
# Following Minio Environmental Variables are used by the nessie and AWS bundle jars to access Minio
# These environment variables need to be created in the container from where Spark scripts will be fired. 
# In this case Spark Master and Workers are used to execute the spark scripts and all the required jars including Unity Catalog and AWS S3 SDK jars are included in this container.
# Since Minio is AWS S3 compatible and AWS S3 SDK jars are used to access it therefore AWS S3 environmental variables are used here.
# and Unity Catalog access Minio through AWS S3 bundle jar which uses environment variables present in the docker container for region and credentials.

# AWS_REGION environment variable will store region of minio
ENV AWS_REGION="us-east-1" 
# Minio username
ENV AWS_ACCESS_KEY_ID="root"  
# Minio password
ENV AWS_SECRET_ACCESS_KEY="jerinminioserver"   
