# Jupyter Pyspark Openjdk 17 image
FROM python:3.11-alpine AS python-base

RUN apk add --no-cache \
    sudo \
    curl \
    vim \
    unzip \
    rsync \
    openjdk17 \
    build-base \
    openssh \
    git \ 
    make \
    g++ \
    bash \
    gcc \
    g++ \
    python3-dev \
    musl-dev \
    make \ 
    linux-headers

# Install Node
RUN apk add --update nodejs npm yarn

## Download spark and hadoop dependencies and install

# Optional env variables
ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}
ENV HADOOP_HOME=${HADOOP_HOME:-"/opt/hadoop"}

RUN mkdir -p ${HADOOP_HOME} && mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}


RUN curl https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz -o spark-3.5.4-bin-hadoop3.tgz \
 && tar xvzf spark-3.5.4-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \
 && rm -rf spark-3.5.4-bin-hadoop3.tgz


FROM python-base AS pyspark

# Install python deps
COPY ./requirements.txt .
RUN pip3 install -r requirements.txt


ENV PATH="/opt/spark/sbin:/opt/spark/bin:${PATH}"
ENV SPARK_HOME="/opt/spark"
ENV SPARK_MASTER="spark://spark-master:7077"
ENV SPARK_MASTER_HOST=spark-master
ENV SPARK_MASTER_PORT=7077

# # Set pyspark environmental variables to python3, whenever Pyspark command is executed in bash shell then it will give python as the default development environment
# ENV PYSPARK_PYTHON=python3
# ENV PYSPARK_DRIVER_PYTHON=python3 

# Set Pyspark Environment Variable to jupyter as IDE and add jupyter's parameters, whenever Pyspark command is executed in bash shell then it will start the jupyter server and give Jupyter IDE as the default development environment
# Execute "jupyter server list" command to get the token to connect to jupyter.
ENV PYSPARK_DRIVER_PYTHON=jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS='lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root'


COPY ./spark-config/spark-defaults.conf "$SPARK_HOME/conf"

RUN chmod u+x /opt/spark/sbin/* && \
    chmod u+x /opt/spark/bin/*

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH


# Define a volume
VOLUME ["/opt/spark/"]

# Create working folder 
RUN mkdir /usr/nessie
RUN mkdir /usr/nessie/nessie-dev/

# Set working dir
WORKDIR /usr/nessie/nessie-dev/


# Start pyspark jupyter
COPY entrypoint-pysparkjupyter.sh .
ENTRYPOINT ["./entrypoint-pysparkjupyter.sh"]

##****************** Very Important to Connect to Minio using AWS S3 Bundle Jars *************** ##
# Following Minio Environmental Variables are used by the nessie and AWS bundle jars to access Minio
# These environment variables need to be created in the container from where Spark scripts will be fired. 
# In this case jupyter is used to execute the spark scripts and all the required jars including nessie jars are included in this container.
# Since Minio is AWS S3 compatible and AWS S3 SDK jars are used to access it therefore AWS S3 environmental variables are used here.
# and Nessie access Minio through AWS S3 bundle jar which uses environment variables present in the docker container for region and credentials.

# AWS_REGION environment variable will store region of minio
ENV AWS_REGION="us-east-1" 
# Minio username
ENV AWS_ACCESS_KEY_ID="root"  
# Minio password
ENV AWS_SECRET_ACCESS_KEY="jerinminioserver"   


# CMD [ "tail","-f","/dev/null" ]