FROM python:3.11.11-bullseye AS spark-base

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      sudo \
      curl \
      vim \
      unzip \
      rsync \
      openjdk-11-jdk \
      build-essential \
      software-properties-common \
      ssh && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*



## Download spark and hadoop dependencies and install

# Optional env variables
ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}
ENV HADOOP_HOME=${HADOOP_HOME:-"/opt/hadoop"}

RUN mkdir -p ${HADOOP_HOME} && mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}

# Install spark
RUN curl https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz -o spark-3.5.4-bin-hadoop3.tgz \
 && tar xvzf spark-3.5.4-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \
 && rm -rf spark-3.5.4-bin-hadoop3.tgz

######### Spark 3.5.4 supports Hive 2.3.9 by default and the latest version of Hive gives error during connection with hive extername metastore.
######### Also by default, Spark 3.5.4 tar has Hive 2.3.9 jars available in its directory.
######### Due to above reason, following code is commented.


# Install Hive for Hive 4.0.1 jars: These hive jars needs to be copied to spark jars folder

# RUN mkdir /opt/hive 

# RUN curl https://dlcdn.apache.org/hive/hive-4.0.1/apache-hive-4.0.1-bin.tar.gz -o apache-hive-4.0.1-bin.tar.gz \
#  && tar xvzf apache-hive-4.0.1-bin.tar.gz --directory /opt/hive --strip-components 1 \
#  && rm -rf apache-hive-4.0.1-bin.tar.gz

# # Remove all hive jars from Spark jar folder
# RUN rm -f /opt/spark/jars/hive*.jar

# # Copy all hive 4.0.1 jars from hive folder to Spark jar folder
# RUN cp /opt/hive/lib/hive*.jar /opt/spark/jars/

FROM spark-base AS pyspark

# Install python deps
COPY ./requirements.txt .
RUN pip3 install -r requirements.txt

ENV PATH="/opt/spark/sbin:/opt/spark/bin:${PATH}"
ENV SPARK_HOME="/opt/spark"
ENV SPARK_MASTER="spark://spark-master:7077"
ENV SPARK_MASTER_HOST=spark-master
ENV SPARK_MASTER_PORT=7077

# # Set pyspark environmental variables to python3, whenever Pyspark command is executed in bash shell then it will give python as the default development environment
# ENV PYSPARK_PYTHON=python3
# ENV PYSPARK_DRIVER_PYTHON=python3 

# Set Pyspark Environment Variable to jupyter as IDE and add jupyter's parameters, whenever Pyspark command is executed in bash shell then it will start the jupyter server and give Jupyter IDE as the default development environment
# Execute "jupyter server list" command to get the token to connect to jupyter.
ENV PYSPARK_DRIVER_PYTHON=jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS='lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root'


# Copy Spark Defaults
COPY ./spark-config/spark-defaults.conf "$SPARK_HOME/conf"


# Add Hive default configs, Postgres DB as external metastore and Minio as Object Storeage in Hive
COPY ./spark-config/hive-site.xml "$SPARK_HOME/conf"


# Download Postgresql jar to connect postgres which will act as hive external metastore
RUN curl https://jdbc.postgresql.org/download/postgresql-42.7.5.jar -o /opt/spark/jars/postgresql-42.7.5.jar

# # Install hadoop aws s3 dependencies for minio
# RUN curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar -o /opt/spark/jars/hadoop-aws-3.4.1.jar
# RUN curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.780/aws-java-sdk-bundle-1.12.780.jar -o /opt/spark/jars/aws-java-sdk-bundle-1.12.780.jar



RUN chmod u+x /opt/spark/sbin/* && \
    chmod u+x /opt/spark/bin/*

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH


# Define a volume
VOLUME ["/opt/spark/"]

# Create work directory 
RUN mkdir /usr/hive-metastore
RUN mkdir /usr/hive-metastore/spark-apps
RUN mkdir /usr/hive-metastore/logs

RUN chmod -R 777 /usr/hive-metastore

WORKDIR /usr/hive-metastore

# Entrypoint script - It should be declared after the WORKDIR instruction, while creating the container, entrypoint script will be executed from the Working Directory.
COPY entrypoint.sh .

ENTRYPOINT ["./entrypoint.sh"]


##****************** Very Important to Connect to Minio using AWS S3 Bundle Jars *************** ##
# Following Minio Environmental Variables are used by the nessie and AWS bundle jars to access Minio
# These environment variables need to be created in the container from where Spark scripts will be fired. 
# In this case Spark Master and Workers are used to execute the spark scripts and all the required jars including Unity Catalog and AWS S3 SDK jars are included in this container.
# Since Minio is AWS S3 compatible and AWS S3 SDK jars are used to access it therefore AWS S3 environmental variables are used here.
# and Unity Catalog access Minio through AWS S3 bundle jar which uses environment variables present in the docker container for region and credentials.

# AWS_REGION environment variable will store region of minio
ENV AWS_REGION="us-east-1" 
# Minio username
ENV AWS_ACCESS_KEY_ID="root"  
# Minio password
ENV AWS_SECRET_ACCESS_KEY="jerinminioserver"   
