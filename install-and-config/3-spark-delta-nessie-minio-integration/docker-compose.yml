version: '3.9'

services:
  spark-master:
    container_name: container-spark-master
    build: 
      context: .
      dockerfile: Dockerfile.spark
    image: spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      # - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/data:/usr/nessie/nessie-dev/data"
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/spark_apps:/usr/nessie/nessie-dev/spark-apps"
      - "spark-logs:/opt/spark/spark-events"
    env_file:
      - ./spark-config/.env.spark

    ports:
      - '9090:8080' # Ued by the Spark Master Web UI
      - '7077:7077' # Used by Spark's cluster manager for accepting connections from workers and applications


  spark-history-server:
    container_name: container-spark-history
    image: spark-image
    entrypoint: ['./entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - ./spark-config/.env.spark
    volumes:
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'

  spark-worker:
#    container_name: da-spark-worker
    image: spark-image
    entrypoint: ['./entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - ./spark-config/.env.spark
    volumes:
      # - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/data:/usr/nessie/nessie-dev/data"
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/spark_apps:/usr/nessie/nessie-dev/spark-apps"
      - "spark-logs:/opt/spark/spark-events"

  minio:
    container_name: container-minio-storage
    build: 
      context: .
      dockerfile: Dockerfile.minio
    image: minio_storage  
    ports:
      - "9000:9000"
      - "9001:9001" 
    volumes:
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/data:/usr/nessie/nessie-dev/data"
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/spark_apps:/usr/nessie/nessie-dev/spark-apps"
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/logs:/usr/nessie/nessie-dev/logs" 

  nessie:
    container_name: container-nessie
    build: 
      context: .
      dockerfile: Dockerfile.nessie
    image: nessie-image
    depends_on:
      - spark-master 
    volumes:
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/spark_apps:/usr/nessie/nessie-dev/spark-apps" 
    ports:
      - '19120:19120' # Used for Nessie UI
      - '19000:9000'

  jupyter-pyspark:
    container_name: container-jupyter-pyspark
    build: 
      context: .
      dockerfile: Dockerfile.pysparkjupyter
    image: jupyter-pyspark-image
    depends_on:
      - spark-master
    env_file:
      - ./spark-config/.env.spark
    volumes:
      # - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/data:/usr/nessie/nessie-dev/data"
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/spark_apps:/usr/nessie/nessie-dev/spark-apps"
      - "/mnt/c/Users/jerin/OneDrive\ -\ Aligned\ Automation/Work/Learn/Tech Solutions/spark-deltalake-unitiycatalog-learn-hands-on/spark-working-folder/logs:/usr/nessie/nessie-dev/logs" 
      - "spark-logs:/opt/spark/spark-events"
    ports: 
      - '8888:8888' # Used for Jupyter Lab
      - '8889:8889' # Used for Jupyter Lab
volumes:
  spark-logs: