FROM python:3.11.11-bullseye AS spark-base

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      sudo \
      curl \
      vim \
      unzip \
      rsync \
      openjdk-11-jdk \
      build-essential \
      software-properties-common \
      ssh && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*



## Download spark and hadoop dependencies and install

# Optional env variables
ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}
ENV HADOOP_HOME=${HADOOP_HOME:-"/opt/hadoop"}

RUN mkdir -p ${HADOOP_HOME} && mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}


RUN curl https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz -o spark-3.5.4-bin-hadoop3.tgz \
 && tar xvzf spark-3.5.4-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \
 && rm -rf spark-3.5.4-bin-hadoop3.tgz


FROM spark-base AS pyspark

# Install python deps
COPY ./requirements.txt .
RUN pip3 install -r requirements.txt

ENV PATH="/opt/spark/sbin:/opt/spark/bin:${PATH}"
ENV SPARK_HOME="/opt/spark"
ENV SPARK_MASTER="spark://spark-master:7077"
ENV SPARK_MASTER_HOST=spark-master
ENV SPARK_MASTER_PORT=7077

# set pyspark environmental variables like python3, jupyter as IDE and jupyter's parameters under 
# ENV PYSPARK_PYTHON=python3
# ENV PYSPARK_DRIVER_PYTHON=python3 
ENV PYSPARK_DRIVER_PYTHON=jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS='lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root'


COPY ./spark-config/spark-defaults.conf "$SPARK_HOME/conf"

RUN chmod u+x /opt/spark/sbin/* && \
    chmod u+x /opt/spark/bin/*

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH


# Define a volume
VOLUME ["/opt/spark/"]

COPY entrypoint.sh .

ENTRYPOINT ["./entrypoint.sh"]


##****************** Very Important to Connect to Minio using AWS S3 Bundle Jars *************** ##
# Following Minio Environmental Variables are used by the nessie and AWS bundle jars to access Minio
# These environment variables need to be created in the container from where Spark scripts will be fired. 
# In this case Spark Master and Workers are used to execute the spark scripts and all the required jars including Unity Catalog and AWS S3 SDK jars are included in this container.
# Since Minio is AWS S3 compatible and AWS S3 SDK jars are used to access it therefore AWS S3 environmental variables are used here.
# and Unity Catalog access Minio through AWS S3 bundle jar which uses environment variables present in the docker container for region and credentials.

# AWS_REGION environment variable will store region of minio
ENV AWS_REGION="us-east-1" 
# Minio username
ENV AWS_ACCESS_KEY_ID="root"  
# Minio password
ENV AWS_SECRET_ACCESS_KEY="jerinminioserver"   
